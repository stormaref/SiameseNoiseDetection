{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets.mnist import FashionMNIST\n",
    "from torchvision.models import DenseNet121_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTPairs(Dataset):\n",
    "    def __init__(self, dataset, num_pairs_per_epoch=100000):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "            #                      std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.length = len(dataset)\n",
    "        self.num_pairs_per_epoch = num_pairs_per_epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_pairs_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = random.sample(range(self.length), 2)\n",
    "        \n",
    "        img1, label1 = self.dataset[i]\n",
    "        img2, label2 = self.dataset[j]\n",
    "        img1 = self.transform(img1)\n",
    "        img2 = self.transform(img2)\n",
    "        label = int(label1 == label2)\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        base_model = models.densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.fc = nn.Linear(224*224, 128)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.feature_extractor(input1)\n",
    "        output1 = self.fc(output1.view(output1.size(0), -1))\n",
    "        output2 = self.feature_extractor(input2)\n",
    "        output2 = self.fc(output2.view(output2.size(0), -1))\n",
    "        return output1, output2\n",
    "\n",
    "    def extract_features(self, input):\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(input)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, dataloader, device):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for img1, img2, label in self.dataloader:\n",
    "                img1, img2, label = img1.to(self.device), img2.to(self.device), label.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output1, output2 = self.model(img1, img2)\n",
    "                loss = self.criterion(output1, output2, label)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(self.dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVisualizer:\n",
    "    def __init__(self, model, dataloader, device):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    def extract_embeddings(self):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for img, _, label in self.dataloader:\n",
    "                img = img.to(self.device)\n",
    "                embedding = self.model.extract_features(img)\n",
    "                embeddings.append(embedding.cpu().numpy())\n",
    "                labels.append(label.cpu().numpy())\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "        return embeddings, labels\n",
    "    \n",
    "    def visualize(self, embeddings, labels):\n",
    "        tsne = TSNE(n_components=2)\n",
    "        tsne_results = tsne.fit_transform(embeddings)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.title('t-SNE visualization of image embeddings')\n",
    "        plt.xlabel('Dimension 1')\n",
    "        plt.ylabel('Dimension 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self, model, dataloader, device):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    def test(self):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for img1, img2, label in self.dataloader:\n",
    "                img1, img2, label = img1.to(self.device), img2.to(self.device), label.to(self.device)\n",
    "                output1, output2 = self.model(img1, img2)\n",
    "                euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "                predictions = (euclidean_distance < 0.5).float()\n",
    "                \n",
    "                all_labels.extend(label.cpu().numpy())\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision = precision_score(all_labels, all_predictions)\n",
    "        recall = recall_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"Test Precision: {precision:.2f}\")\n",
    "        print(f\"Test Recall: {recall:.2f}\")\n",
    "        print(f\"Test F1 Score: {f1:.2f}\")\n",
    "\n",
    "        return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "starting training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, criterion, optimizer, train_loader, device)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting testing\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img1, img2, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader:\n\u001b[1;32m     15\u001b[0m     img1, img2, label \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), img2\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), label\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     output1, output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(img1, img2)\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output1, output2, label)\n",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img1, img2, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader:\n\u001b[1;32m     15\u001b[0m     img1, img2, label \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), img2\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), label\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     output1, output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(img1, img2)\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output1, output2, label)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using device:', device)\n",
    "# Load Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n",
    "\n",
    "train_pairs_dataset = FashionMNISTPairs(train_dataset)\n",
    "test_pairs_dataset = FashionMNISTPairs(test_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_pairs_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_pairs_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SiameseNetwork()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "print('starting training')\n",
    "trainer = Trainer(model, criterion, optimizer, train_loader, device)\n",
    "trainer.train(num_epochs=10)\n",
    "\n",
    "# Test the model\n",
    "print('starting testing')\n",
    "tester = Tester(model, test_loader, device)\n",
    "tester.test()\n",
    "\n",
    "# Visualize embeddings\n",
    "visualizer = EmbeddingVisualizer(model, test_loader, device)\n",
    "embeddings, labels = visualizer.extract_embeddings()\n",
    "visualizer.visualize(embeddings, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
