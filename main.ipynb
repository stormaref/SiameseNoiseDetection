{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets.mnist import FashionMNIST\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTPairs(Dataset):\n",
    "    def __init__(self, dataset: FashionMNIST, num_pairs_per_epoch=100000):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3), # Convert 1 channel to 3 channels\n",
    "            transforms.Resize((224, 224)), # Resize to the size expected by ResNet18\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.length = len(dataset)\n",
    "        self.num_pairs_per_epoch = num_pairs_per_epoch\n",
    "        self.pairs_indices = self.generate_pairs_indices()\n",
    "\n",
    "    def generate_pairs_indices(self):\n",
    "        pairs_indices = []\n",
    "        for _ in range(self.num_pairs_per_epoch):\n",
    "            i, j = random.sample(range(self.length), 2)\n",
    "            pairs_indices.append((i, j))\n",
    "        return pairs_indices\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_pairs_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.pairs_indices[idx]\n",
    "        img1, label1 = self.dataset[i]\n",
    "        img2, label2 = self.dataset[j]\n",
    "        img1 = self.transform(img1)\n",
    "        img2 = self.transform(img2)\n",
    "        return img1, img2, torch.tensor(label1), torch.tensor(label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        base_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.fc_embedding = nn.Linear(512, 128)  # Embedding layer for Siamese Network\n",
    "        self.fc_classifier = nn.Linear(128, num_classes)  # Classifier layer\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Feature extraction\n",
    "        feat1 = self.feature_extractor(input1)\n",
    "        feat2 = self.feature_extractor(input2)\n",
    "\n",
    "        # Flatten feature maps\n",
    "        feat1 = feat1.view(feat1.size(0), -1)\n",
    "        feat2 = feat2.view(feat2.size(0), -1)\n",
    "\n",
    "        # Embedding\n",
    "        emb1 = self.fc_embedding(feat1)\n",
    "        emb2 = self.fc_embedding(feat2)\n",
    "\n",
    "        # Classification\n",
    "        class1 = self.fc_classifier(emb1)\n",
    "        class2 = self.fc_classifier(emb2)\n",
    "\n",
    "        return emb1, emb2, class1, class2\n",
    "\n",
    "    def extract_features(self, input):\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(input)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            features = self.fc_embedding(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, contrastive_criterion, classifier_criterion, optimizer, dataloader, device):\n",
    "        self.model = model\n",
    "        self.contrastive_criterion = contrastive_criterion\n",
    "        self.classifier_criterion = classifier_criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.epoch_losses = []\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            progress_bar = tqdm(self.dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            for img1, img2, label1, label2 in progress_bar:\n",
    "                img1, img2, label1, label2 = img1.to(self.device), img2.to(self.device), label1.to(self.device), label2.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                emb1, emb2, class1, class2 = self.model(img1, img2)\n",
    "                \n",
    "                # Calculate same_label dynamically\n",
    "                same_label = (label1 == label2).float()\n",
    "                \n",
    "                # Calculate losses\n",
    "                contrastive_loss = self.contrastive_criterion(emb1, emb2, same_label)\n",
    "                classifier_loss1 = self.classifier_criterion(class1, label1)\n",
    "                classifier_loss2 = self.classifier_criterion(class2, label2)\n",
    "                total_loss = contrastive_loss + classifier_loss1 + classifier_loss2\n",
    "                \n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += total_loss.item()\n",
    "                progress_bar.set_postfix(loss=total_loss.item())\n",
    "            avg_epoch_loss = epoch_loss / len(self.dataloader)\n",
    "            self.epoch_losses.append(avg_epoch_loss)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss}\")\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.epoch_losses, label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVisualizer:\n",
    "    def __init__(self, model, dataloader, device):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    def extract_embeddings(self):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for img, _, label in self.dataloader:\n",
    "                img = img.to(self.device)\n",
    "                embedding = self.model.extract_features(img)\n",
    "                embeddings.append(embedding.cpu().numpy())\n",
    "                labels.append(label.cpu().numpy())\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "        return embeddings, labels\n",
    "    \n",
    "    def visualize(self, embeddings, labels):\n",
    "        tsne = TSNE(n_components=2)\n",
    "        tsne_results = tsne.fit_transform(embeddings)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.title('t-SNE visualization of image embeddings')\n",
    "        plt.xlabel('Dimension 1')\n",
    "        plt.ylabel('Dimension 2')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self, model, dataloader, device):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "    def test(self):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        all_labels = []\n",
    "        all_predictions1 = []\n",
    "        all_predictions2 = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for img1, img2, label1, label2 in tqdm(self.dataloader, desc=\"Testing\"):\n",
    "                img1, img2, label1, label2 = img1.to(self.device), img2.to(self.device), label1.to(self.device), label2.to(self.device)\n",
    "                \n",
    "                emb1, emb2, class1, class2 = self.model(img1, img2)\n",
    "                \n",
    "                _, pred1 = torch.max(class1, 1)\n",
    "                _, pred2 = torch.max(class2, 1)\n",
    "                \n",
    "                all_labels.extend(label1.cpu().numpy())\n",
    "                all_labels.extend(label2.cpu().numpy())\n",
    "                all_predictions1.extend(pred1.cpu().numpy())\n",
    "                all_predictions2.extend(pred2.cpu().numpy())\n",
    "\n",
    "        all_predictions = np.concatenate([all_predictions1, all_predictions2])\n",
    "        all_labels = np.array(all_labels)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "        recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"Test Precision: {precision:.2f}\")\n",
    "        print(f\"Test Recall: {recall:.2f}\")\n",
    "        print(f\"Test F1 Score: {f1:.2f}\")\n",
    "\n",
    "        return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using device:', device)\n",
    "# Load Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n",
    "\n",
    "train_pairs_dataset = FashionMNISTPairs(train_dataset, num_pairs_per_epoch=25000)\n",
    "test_pairs_dataset = FashionMNISTPairs(test_dataset, num_pairs_per_epoch=2500)\n",
    "\n",
    "train_loader = DataLoader(train_pairs_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_pairs_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SiameseNetwork()\n",
    "contrastive_criterion = ContrastiveLoss()\n",
    "classifier_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "print('starting training')\n",
    "trainer = Trainer(model,contrastive_criterion, classifier_criterion, optimizer, train_loader, device)\n",
    "trainer.train(num_epochs=20)\n",
    "\n",
    "# Plot the training loss\n",
    "trainer.plot_losses()\n",
    "\n",
    "# Test the model\n",
    "print('starting testing')\n",
    "tester = Tester(model, test_loader, device)\n",
    "tester.test()\n",
    "\n",
    "# Visualize embeddings\n",
    "visualizer = EmbeddingVisualizer(model, test_loader, device)\n",
    "embeddings, labels = visualizer.extract_embeddings()\n",
    "visualizer.visualize(embeddings, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
